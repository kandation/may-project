{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_uri = 'D:\\Work\\Home-2023\\may-project\\labs\\outputsx\\merged\\merged_บ้านสบป้าด.csv.csv'\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_uri)\n",
    "\n",
    "# Select relevant features (assuming 'SO2' is your target)\n",
    "features = ['Value_TEMP', 'Value_RH', 'Value_SO2', 'Value_NO2', 'Value_PM10']\n",
    "data_selected = data[features]\n",
    "\n",
    "# Fill missing values if needed\n",
    "data_selected = data_selected.interpolate(method='linear')\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data_selected)\n",
    "\n",
    "# Function to create dataset for LSTM\n",
    "def create_dataset(dataset, look_back=48):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), 0:-1]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, -1])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# Prepare data for LSTM\n",
    "look_back = 48\n",
    "X, y = create_dataset(data_scaled, look_back)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1169/1169 - 16s - loss: 0.0040 - val_loss: 0.0032 - 16s/epoch - 14ms/step\n",
      "Epoch 2/20\n",
      "1169/1169 - 12s - loss: 0.0033 - val_loss: 0.0030 - 12s/epoch - 10ms/step\n",
      "Epoch 3/20\n",
      "1169/1169 - 12s - loss: 0.0031 - val_loss: 0.0029 - 12s/epoch - 11ms/step\n",
      "Epoch 4/20\n",
      "1169/1169 - 13s - loss: 0.0030 - val_loss: 0.0029 - 13s/epoch - 11ms/step\n",
      "Epoch 5/20\n",
      "1169/1169 - 13s - loss: 0.0028 - val_loss: 0.0028 - 13s/epoch - 11ms/step\n",
      "Epoch 6/20\n",
      "1169/1169 - 13s - loss: 0.0027 - val_loss: 0.0026 - 13s/epoch - 11ms/step\n",
      "Epoch 7/20\n",
      "1169/1169 - 13s - loss: 0.0025 - val_loss: 0.0025 - 13s/epoch - 11ms/step\n",
      "Epoch 8/20\n",
      "1169/1169 - 13s - loss: 0.0024 - val_loss: 0.0023 - 13s/epoch - 11ms/step\n",
      "Epoch 9/20\n",
      "1169/1169 - 14s - loss: 0.0023 - val_loss: 0.0022 - 14s/epoch - 12ms/step\n",
      "Epoch 10/20\n",
      "1169/1169 - 13s - loss: 0.0022 - val_loss: 0.0021 - 13s/epoch - 12ms/step\n",
      "Epoch 11/20\n",
      "1169/1169 - 13s - loss: 0.0021 - val_loss: 0.0021 - 13s/epoch - 11ms/step\n",
      "Epoch 12/20\n",
      "1169/1169 - 13s - loss: 0.0021 - val_loss: 0.0021 - 13s/epoch - 11ms/step\n",
      "Epoch 13/20\n",
      "1169/1169 - 13s - loss: 0.0020 - val_loss: 0.0021 - 13s/epoch - 11ms/step\n",
      "Epoch 14/20\n",
      "1169/1169 - 12s - loss: 0.0020 - val_loss: 0.0020 - 12s/epoch - 10ms/step\n",
      "Epoch 15/20\n",
      "1169/1169 - 12s - loss: 0.0020 - val_loss: 0.0020 - 12s/epoch - 11ms/step\n",
      "Epoch 16/20\n",
      "1169/1169 - 12s - loss: 0.0019 - val_loss: 0.0020 - 12s/epoch - 10ms/step\n",
      "Epoch 17/20\n",
      "1169/1169 - 14s - loss: 0.0019 - val_loss: 0.0019 - 14s/epoch - 12ms/step\n",
      "Epoch 18/20\n",
      "1169/1169 - 14s - loss: 0.0019 - val_loss: 0.0019 - 14s/epoch - 12ms/step\n",
      "Epoch 19/20\n",
      "1169/1169 - 12s - loss: 0.0018 - val_loss: 0.0018 - 12s/epoch - 11ms/step\n",
      "Epoch 20/20\n",
      "1169/1169 - 11s - loss: 0.0018 - val_loss: 0.0019 - 11s/epoch - 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x266420a9dc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# GPU configuration (optional, for fine-tuning performance)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Rest of your model code\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2]), activation='tanh', recurrent_activation='sigmoid', return_sequences=True))\n",
    "model.add(LSTM(50, activation='tanh', recurrent_activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 3s 5ms/step\n",
      "Test RMSE: 19.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Invert scaling for prediction - We need to create a dummy array with the same shape as the input features\n",
    "y_pred_rescaled = np.concatenate((np.zeros((y_pred.shape[0], data_scaled.shape[1]-1)), y_pred), axis=1)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred_rescaled)[:, -1]  # Select only the last column (SO2)\n",
    "\n",
    "# Invert scaling for actual values\n",
    "y_test_rescaled = np.concatenate((np.zeros((y_test.shape[0], data_scaled.shape[1]-1)), y_test.reshape(-1, 1)), axis=1)\n",
    "y_test_inv = scaler.inverse_transform(y_test_rescaled)[:, -1]  # Select only the last column (SO2)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "print('Test RMSE: %.3f' % rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Home-2023\\may-project\\labs\\09-lstm-gpt4.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Home-2023/may-project/labs/09-lstm-gpt4.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     predicted_SO2\u001b[39m.\u001b[39mappend(prediction_SO2[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Home-2023/may-project/labs/09-lstm-gpt4.ipynb#W3sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# Update input_data with the new prediction\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Work/Home-2023/may-project/labs/09-lstm-gpt4.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     input_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvstack((input_data[\u001b[39m1\u001b[39;49m:], prediction))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Home-2023/may-project/labs/09-lstm-gpt4.ipynb#W3sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Generate timestamps for the next 48 hours\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Home-2023/may-project/labs/09-lstm-gpt4.ipynb#W3sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m predicted_timestamps \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mdate_range(start\u001b[39m=\u001b[39mdata[\u001b[39m'\u001b[39m\u001b[39mDatetime\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], periods\u001b[39m=\u001b[39m\u001b[39m49\u001b[39m, freq\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mH\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32md:\\Users\\kandation\\me\\anaconda3\\envs\\project-may\\lib\\site-packages\\numpy\\core\\shape_base.py:289\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    288\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[1;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 5 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your original DataFrame and 'model' is your trained LSTM model\n",
    "\n",
    "## Assuming 'data' is your original DataFrame and 'model' is your trained LSTM model\n",
    "\n",
    "predicted_SO2 = []\n",
    "input_data = data_scaled[-look_back:]  # Initial input data\n",
    "\n",
    "for i in range(48):  # Predicting 48 steps\n",
    "    # Reshape and predict\n",
    "    input_data_reshaped = np.reshape(input_data, (1, look_back, data_scaled.shape[1]))\n",
    "    prediction = model.predict(input_data_reshaped)\n",
    "\n",
    "    # Inverse scale the prediction and append to results\n",
    "    prediction_rescaled = np.concatenate((np.zeros((prediction.shape[0], data_scaled.shape[1]-1)), prediction), axis=1)\n",
    "    prediction_SO2 = scaler.inverse_transform(prediction_rescaled)[:, -1]\n",
    "    predicted_SO2.append(prediction_SO2[0])\n",
    "\n",
    "    # Update input_data with the new prediction\n",
    "    input_data = np.vstack((input_data[1:], prediction))\n",
    "\n",
    "# Generate timestamps for the next 48 hours\n",
    "predicted_timestamps = pd.date_range(start=data['Datetime'].iloc[-1], periods=49, freq='H')[1:]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(predicted_timestamps, predicted_SO2, label='Predicted SO2')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('SO2 Levels')\n",
    "plt.title('SO2 Level Prediction for the Next 48 Hours')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-may",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
